{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac035f3-ebbe-4f4f-96ff-d31204564f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d46dde-c118-4333-8960-ba4a1a6fcd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1506 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0/255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'dataset/train',\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'sparse',\n",
    "    #save_to_dir = 'ArgumentedImages'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1a8e45-cfa2-4acd-82f9-c727a151386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.73941505 0.7126037  0.75294125]\n",
      "  [0.7570186  0.7232775  0.783732  ]\n",
      "  [0.730503   0.68820125 0.76187557]\n",
      "  ...\n",
      "  [0.67936784 0.6754463  0.7303482 ]\n",
      "  [0.6909796  0.68705803 0.74196   ]\n",
      "  [0.70259136 0.6986698  0.75357175]]\n",
      "\n",
      " [[0.74143445 0.71613777 0.75294125]\n",
      "  [0.75197005 0.71873385 0.7771688 ]\n",
      "  [0.7395905  0.69880325 0.7709631 ]\n",
      "  ...\n",
      "  [0.771606   0.76768446 0.8225864 ]\n",
      "  [0.78725666 0.7833351  0.8382371 ]\n",
      "  [0.8029073  0.7989857  0.8538877 ]]\n",
      "\n",
      " [[0.7434539  0.7196717  0.75294125]\n",
      "  [0.7469214  0.71419007 0.7706057 ]\n",
      "  [0.74867797 0.7094053  0.7800505 ]\n",
      "  ...\n",
      "  [0.8750746  0.87115306 0.926055  ]\n",
      "  [0.8634628  0.85954124 0.9144432 ]\n",
      "  [0.85185105 0.8479295  0.90283144]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.61506665 0.5954588  0.6229098 ]\n",
      "  [0.61607635 0.5964685  0.6239195 ]\n",
      "  [0.61708605 0.5974782  0.6249292 ]\n",
      "  ...\n",
      "  [0.5874818  0.560282   0.62202233]\n",
      "  [0.5300314  0.5025804  0.5653255 ]\n",
      "  [0.5327005  0.5052495  0.5752048 ]]\n",
      "\n",
      " [[0.6244628  0.60485494 0.6323059 ]\n",
      "  [0.6259774  0.60636955 0.63382053]\n",
      "  [0.62749195 0.6078841  0.6353351 ]\n",
      "  ...\n",
      "  [0.58596724 0.5592722  0.61899316]\n",
      "  [0.53810906 0.5106581  0.5734032 ]\n",
      "  [0.5316907  0.50423974 0.5731854 ]]\n",
      "\n",
      " [[0.61868715 0.5990793  0.6265303 ]\n",
      "  [0.6146483  0.59504044 0.6224914 ]\n",
      "  [0.61060935 0.5910015  0.61845255]\n",
      "  ...\n",
      "  [0.58445257 0.5582625  0.615964  ]\n",
      "  [0.5461868  0.5187358  0.581481  ]\n",
      "  [0.530681   0.50323004 0.5711659 ]]]\n"
     ]
    }
   ],
   "source": [
    "for image_batch, label_batch in train_generator:\n",
    "    print(image_batch[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717c5a12-4e42-464f-ab50-a2d3d43f7e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0/255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    'dataset/val',\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'sparse',\n",
    "    #save_to_dir = 'ArgumentedImages'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c5f7024-2cb4-4ab5-899f-f01915fe9ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 431 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(\n",
    "    rescale = 1.0/255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=10\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'dataset/test',\n",
    "    target_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'sparse',\n",
    "    #save_to_dir = 'ArgumentedImages'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26de1c5a-8a2c-44a6-9d91-489698ee62f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "n_classes = 3\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=input_shape),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)), \n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01753854-9456-419b-a448-7ae134445db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 127, 127, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 62, 62, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 30, 30, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                16448     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 183,747\n",
      "Trainable params: 183,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7a041c-a870-4981-a637-07126e1a80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ea91a6-8a11-43ba-8868-c2d431ee9691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.0625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1506/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4821e981-7193-4549-800d-bce7c43efe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 [==============================] - 28s 531ms/step - loss: 0.9094 - accuracy: 0.4729 - val_loss: 0.8827 - val_accuracy: 0.4531\n",
      "Epoch 2/20\n",
      "47/47 [==============================] - 11s 222ms/step - loss: 0.7773 - accuracy: 0.6079 - val_loss: 0.7210 - val_accuracy: 0.5990\n",
      "Epoch 3/20\n",
      "47/47 [==============================] - 11s 225ms/step - loss: 0.4351 - accuracy: 0.8107 - val_loss: 0.3149 - val_accuracy: 0.8698\n",
      "Epoch 4/20\n",
      "47/47 [==============================] - 11s 225ms/step - loss: 0.2650 - accuracy: 0.9016 - val_loss: 0.2968 - val_accuracy: 0.8906\n",
      "Epoch 5/20\n",
      "47/47 [==============================] - 11s 226ms/step - loss: 0.2457 - accuracy: 0.9009 - val_loss: 0.3433 - val_accuracy: 0.8750\n",
      "Epoch 6/20\n",
      "47/47 [==============================] - 11s 226ms/step - loss: 0.2345 - accuracy: 0.9016 - val_loss: 0.2307 - val_accuracy: 0.9062\n",
      "Epoch 7/20\n",
      "47/47 [==============================] - 11s 228ms/step - loss: 0.1565 - accuracy: 0.9417 - val_loss: 0.1770 - val_accuracy: 0.9375\n",
      "Epoch 8/20\n",
      "47/47 [==============================] - 10s 220ms/step - loss: 0.1533 - accuracy: 0.9342 - val_loss: 0.2547 - val_accuracy: 0.9062\n",
      "Epoch 9/20\n",
      "47/47 [==============================] - 11s 224ms/step - loss: 0.1252 - accuracy: 0.9478 - val_loss: 0.1920 - val_accuracy: 0.9375\n",
      "Epoch 10/20\n",
      "47/47 [==============================] - 10s 221ms/step - loss: 0.1449 - accuracy: 0.9450 - val_loss: 0.1714 - val_accuracy: 0.9375\n",
      "Epoch 11/20\n",
      "47/47 [==============================] - 11s 237ms/step - loss: 0.1019 - accuracy: 0.9634 - val_loss: 0.2042 - val_accuracy: 0.9271\n",
      "Epoch 12/20\n",
      "47/47 [==============================] - 11s 231ms/step - loss: 0.1057 - accuracy: 0.9552 - val_loss: 0.2282 - val_accuracy: 0.9219\n",
      "Epoch 13/20\n",
      "47/47 [==============================] - 11s 228ms/step - loss: 0.1021 - accuracy: 0.9586 - val_loss: 0.1089 - val_accuracy: 0.9583\n",
      "Epoch 14/20\n",
      "47/47 [==============================] - 11s 226ms/step - loss: 0.1210 - accuracy: 0.9545 - val_loss: 0.1484 - val_accuracy: 0.9479\n",
      "Epoch 15/20\n",
      "47/47 [==============================] - 11s 225ms/step - loss: 0.0862 - accuracy: 0.9654 - val_loss: 0.0941 - val_accuracy: 0.9531\n",
      "Epoch 16/20\n",
      "47/47 [==============================] - 11s 223ms/step - loss: 0.1044 - accuracy: 0.9586 - val_loss: 0.1778 - val_accuracy: 0.9323\n",
      "Epoch 17/20\n",
      "47/47 [==============================] - 11s 227ms/step - loss: 0.0823 - accuracy: 0.9681 - val_loss: 0.1352 - val_accuracy: 0.9479\n",
      "Epoch 18/20\n",
      "47/47 [==============================] - 10s 221ms/step - loss: 0.0778 - accuracy: 0.9661 - val_loss: 0.0887 - val_accuracy: 0.9688\n",
      "Epoch 19/20\n",
      "47/47 [==============================] - 10s 220ms/step - loss: 0.1057 - accuracy: 0.9620 - val_loss: 0.1344 - val_accuracy: 0.9531\n",
      "Epoch 20/20\n",
      "47/47 [==============================] - 11s 225ms/step - loss: 0.0712 - accuracy: 0.9735 - val_loss: 0.0908 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26f66b3a500>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator, steps_per_epoch=47, batch_size=32, validation_data=validation_generator, validation_steps=6, verbose=1, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46df7e52-b331-4389-aa99-2338d2f098fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 7s 450ms/step - loss: 0.0994 - accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90a6b10-3f96-41e9-bb5b-fe5482e1fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09936556965112686, 0.9721577763557434]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae065492-1060-4042-a44a-68639bd5ed29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
